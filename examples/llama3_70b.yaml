# Llama3-70B
model:
  n_layers: 80
  hidden_sz: 8192
  inter_sz: 28672
  n_q_heads: 64
  n_kv_heads: 8
  head_dim: 128
  vocab_sz: 128256
  glu: true
  rotary_embeds: true
  dropout: false
  tie_embeddings: true

parallelism:
  tp: 8
  pp: 2
  dp: 64
  vpp: 1
  sp: true
  zero_level: 1

  bucket_size_mb: 250

performance:
  activation_checkpointing_type: selective

data:
  gbs: 2048
  seqlen: 2048
  microbatch_sz: 1

hardware:
  node_type: p4d.24xlarge
