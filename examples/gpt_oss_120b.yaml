model:
  n_layers: 36
  hidden_sz: 2880
  inter_sz: 2880
  n_q_heads: 64
  n_kv_heads: 8
  head_dim: 64
  vocab_sz: 201088
  glu: true
  rotary_embeds: true
  dropout: false
  tie_embeddings: false

  # MoE specific parameters
  moe:
    n_experts: 128
    expert_inter_sz: 2880
    experts_per_token: 4
    capacity_factor: 1.0
    moe_frequency: 1.0
    expert_tp_degree: 1

parallelism:
  tp: 8
  pp: 6
  dp: 128
  ep: 8
  vpp: 6
  sp: true
  zero_level: 1

  n_param_buckets: 5

performance:
  activation_checkpointing_type: selective

data:
  gbs: 8192
  seqlen: 4096
  microbatch_sz: 1

hardware:
  node_type: p4d.24xlarge
